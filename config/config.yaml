
datasets: [
  # '/home/valiantsec/phb/phb_research/data_2/C_sample.jsonl', # 0.8
  # '/home/valiantsec/phb/phb_research/data_2/C++_sample.jsonl',
  # '/home/valiantsec/phb/phb_research/data_2/Java_sample.jsonl',
  '/home/valiantsec/phb/phb_research/data_2/Python_sample.jsonl'
]
num_samples: 100000

# Model Configuration
model_path: "/home/valiantsec/phb/models/flan-t5-small"
# model_path: "/home/valiantsec/phb/models/codet5"
  # 或您选择的其他T5模型路径

# Dataset Configuration
max_length: 32  # 输入序列的最大长度

# Training Configuration
output_dir: "./results"
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
learning_rate: 2e-4
lr_scheduler_type: "cosine"
warmup_ratio: 0.05
num_train_epochs: 4

# Saving and Logging Configuration
save_strategy: "steps"
save_steps: 500
logging_steps: 100
evaluation_strategy: "steps"
eval_steps: 500

# LoRA Configuration
lora_r: 8
lora_alpha: 32
lora_dropout: 0.05